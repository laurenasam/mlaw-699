{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Case Number                                       Link to H-2A  \\\n",
      "0  H-300-21280-631097  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "1  H-300-21202-474481  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "2  H-300-21195-461870  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "3  H-300-21183-441615  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "4  H-300-20255-817429  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "\n",
      "  Hit or No Hit Market Farm Narratives  \\\n",
      "0        No Hit    NaN  NaN        NaN   \n",
      "1        No Hit    NaN  NaN        NaN   \n",
      "2        No Hit    NaN  NaN        NaN   \n",
      "3        No Hit    NaN  NaN        NaN   \n",
      "4        No Hit    NaN  NaN        NaN   \n",
      "\n",
      "  Facebook. Google Reviews, Website, LinkedIn, Google Maps, Instagram, etc.???  \\\n",
      "0                                                NaN                             \n",
      "1                                                NaN                             \n",
      "2                                                NaN                             \n",
      "3                                                NaN                             \n",
      "4                                                NaN                             \n",
      "\n",
      "   Certainty    year      Recruiter Hits - By name or by logo Person  \n",
      "0        NaN  2022.0  D Mendoza - 1                       NaN    NaN  \n",
      "1        NaN  2021.0  D Mendoza - 1                       NaN    NaN  \n",
      "2        NaN  2021.0  D Mendoza - 1                       NaN    NaN  \n",
      "3        NaN  2021.0  D Mendoza - 1                       NaN    NaN  \n",
      "4        NaN  2021.0  D Mendoza - 1                       NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from csv\n",
    "data = pd.read_csv('Master Document of H-2A Orders - Hit or Not - Previous Semester H-2A Orders Successes.csv')\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Case Number                                       Link to H-2A  \\\n",
      "13  H-300-21029-042637  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "35  H-300-20013-248635  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "38  H-300-20002-228933  https://seasonaljobs.dol.gov/api/job-order/H-3...   \n",
      "44  H-300-19044-646927  https://seasonaljobs.dol.gov/jobs/H-300-19044-...   \n",
      "48  H-300-19114-230380  https://seasonaljobs.dol.gov/jobs/H-300-19114-...   \n",
      "\n",
      "   Hit or No Hit                                             Market  \\\n",
      "13           Hit                        Kroger, Walmart, Instacart    \n",
      "35           Hit  Naturipe Farms, 99 Cents Only Stores, Ahold, A...   \n",
      "38           Hit  Store Locator – sealtheseasons, Peoples-Food A...   \n",
      "44           Hit                             https://wishfarms.com/   \n",
      "48           Hit                                      Harris Teeter   \n",
      "\n",
      "                             Farm  \\\n",
      "13                  Haigler Farms   \n",
      "35         Down South Berries LLC   \n",
      "38                       L&S Farm   \n",
      "44  Major League Blueberries, LLC   \n",
      "48       Sleepy Creek Farms, Inc.   \n",
      "\n",
      "                                           Narratives  \\\n",
      "13  I googled the worksite address on the work ord...   \n",
      "35  H2A list > work order lists Down South Berries...   \n",
      "38  Googled: L&S Farm at 213 Pine Cone Road, Alma,...   \n",
      "44  H2A list > Job order on DOL site, listed busin...   \n",
      "48  H2A > job order work site address > googled an...   \n",
      "\n",
      "   Facebook. Google Reviews, Website, LinkedIn, Google Maps, Instagram, etc.???  \\\n",
      "13                                            Website                             \n",
      "35  website, but also relied on info gained over a...                             \n",
      "38                            Websites, including SoS                             \n",
      "44                                            Website                             \n",
      "48                                     Google reviews                             \n",
      "\n",
      "    Certainty    year      Recruiter Hits - By name or by logo Person  \n",
      "13        3.0  2021.0  D Mendoza - 1                      Both    Ben  \n",
      "35        1.0  2020.0  D Mendoza - 1                      Both    Ben  \n",
      "38        1.0  2020.0  D Mendoza - 1                      Both    Ben  \n",
      "44        3.0  2019.0  D Mendoza - 1                      Logo    Ben  \n",
      "48        2.0  2019.0  D Mendoza - 1                      Name    Ben  \n"
     ]
    }
   ],
   "source": [
    "# make a df of just rows with \"hit\"\n",
    "hit = data[data['Hit or No Hit'] == 'Hit']\n",
    "\n",
    "# print the first 5 rows of the hit data\n",
    "print(hit.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the hit data to a new csv\n",
    "hit.to_csv('hit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/shirleyaraizasantaella/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Downloaded: H2A_PDFs/H-300-20013-248635.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21029-042637.pdf\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-19044-646927 (Content-Type: text/html; charset=utf-8)\n",
      "✅ Downloaded: H2A_PDFs/H-300-20002-228933.pdf\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-19114-230380 (Content-Type: text/html; charset=utf-8)\n",
      "✅ Downloaded: H2A_PDFs/H-300-22014-831909.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21006-998351.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20357-975271.pdf\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-19068-891355 (Content-Type: text/html; charset=utf-8)\n",
      "✅ Downloaded: H2A_PDFs/H-300-20009-241352.pdf\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-19106-808682 (Content-Type: text/html; charset=utf-8)\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-18254-022527 (Content-Type: text/html; charset=utf-8)\n",
      "⚠️ Not a PDF: https://seasonaljobs.dol.gov/jobs/H-300-19051-356435 (Content-Type: text/html; charset=utf-8)\n",
      "✅ Downloaded: H2A_PDFs/104779_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/99257_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/115969_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-23086-881434.pdf\n",
      "⚠️ No 'View Job Order' PDF found on webpage: https://api.seasonaljobs.dol.gov/job-order/H-300-23028-734237\n",
      "✅ Downloaded: H2A_PDFs/115829_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/117179_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/74479_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/66774_order.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20260-827359.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21253-577257.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20363-980683.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21007-999949.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21039-063180.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21026-033893.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21124-285763.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20208-735658.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-21166-398136.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20150-611826.pdf\n",
      "✅ Downloaded: H2A_PDFs/H-300-20051-337029.pdf\n",
      "\n",
      "📊 **Download Summary**\n",
      "✅ Successfully downloaded PDFs: 26\n",
      "❌ Failed URLs: 7\n",
      "\n",
      "✅ **Successfully downloaded PDFs:**\n",
      "H2A_PDFs/H-300-20013-248635.pdf\n",
      "H2A_PDFs/H-300-21029-042637.pdf\n",
      "H2A_PDFs/H-300-20002-228933.pdf\n",
      "H2A_PDFs/H-300-22014-831909.pdf\n",
      "H2A_PDFs/H-300-21006-998351.pdf\n",
      "H2A_PDFs/H-300-20357-975271.pdf\n",
      "H2A_PDFs/H-300-20009-241352.pdf\n",
      "H2A_PDFs/104779_order.pdf\n",
      "H2A_PDFs/99257_order.pdf\n",
      "H2A_PDFs/115969_order.pdf\n",
      "H2A_PDFs/H-300-23086-881434.pdf\n",
      "H2A_PDFs/115829_order.pdf\n",
      "H2A_PDFs/117179_order.pdf\n",
      "H2A_PDFs/74479_order.pdf\n",
      "H2A_PDFs/66774_order.pdf\n",
      "H2A_PDFs/H-300-20260-827359.pdf\n",
      "H2A_PDFs/H-300-21253-577257.pdf\n",
      "H2A_PDFs/H-300-20363-980683.pdf\n",
      "H2A_PDFs/H-300-21007-999949.pdf\n",
      "H2A_PDFs/H-300-21039-063180.pdf\n",
      "H2A_PDFs/H-300-21026-033893.pdf\n",
      "H2A_PDFs/H-300-21124-285763.pdf\n",
      "H2A_PDFs/H-300-20208-735658.pdf\n",
      "H2A_PDFs/H-300-21166-398136.pdf\n",
      "H2A_PDFs/H-300-20150-611826.pdf\n",
      "H2A_PDFs/H-300-20051-337029.pdf\n",
      "\n",
      "⚠️ **Failed to download PDFs from these URLs:**\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-19044-646927\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-19114-230380\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-19068-891355\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-19106-808682\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-18254-022527\n",
      "https://seasonaljobs.dol.gov/jobs/H-300-19051-356435\n",
      "https://api.seasonaljobs.dol.gov/job-order/H-300-23028-734237\n",
      "\n",
      "🔍 Debugging successful_downloads list:\n",
      "['H2A_PDFs/H-300-20013-248635.pdf', 'H2A_PDFs/H-300-21029-042637.pdf', 'H2A_PDFs/H-300-20002-228933.pdf', 'H2A_PDFs/H-300-22014-831909.pdf', 'H2A_PDFs/H-300-21006-998351.pdf', 'H2A_PDFs/H-300-20357-975271.pdf', 'H2A_PDFs/H-300-20009-241352.pdf', 'H2A_PDFs/104779_order.pdf', 'H2A_PDFs/99257_order.pdf', 'H2A_PDFs/115969_order.pdf', 'H2A_PDFs/H-300-23086-881434.pdf', 'H2A_PDFs/115829_order.pdf', 'H2A_PDFs/117179_order.pdf', 'H2A_PDFs/74479_order.pdf', 'H2A_PDFs/66774_order.pdf', 'H2A_PDFs/H-300-20260-827359.pdf', 'H2A_PDFs/H-300-21253-577257.pdf', 'H2A_PDFs/H-300-20363-980683.pdf', 'H2A_PDFs/H-300-21007-999949.pdf', 'H2A_PDFs/H-300-21039-063180.pdf', 'H2A_PDFs/H-300-21026-033893.pdf', 'H2A_PDFs/H-300-21124-285763.pdf', 'H2A_PDFs/H-300-20208-735658.pdf', 'H2A_PDFs/H-300-21166-398136.pdf', 'H2A_PDFs/H-300-20150-611826.pdf', 'H2A_PDFs/H-300-20051-337029.pdf']\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium webdriver-manager beautifulsoup4 pandas requests\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"hit.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Column containing URLs\n",
    "column_name = \"Link to H-2A\"\n",
    "urls = df[column_name].dropna().unique()\n",
    "\n",
    "# Create folder for PDFs\n",
    "output_folder = \"H2A_PDFs\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set up Chrome WebDriver (Headless Mode)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--log-level=3\")  \n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Track successes and failures\n",
    "successful_downloads = []\n",
    "failed_urls = []\n",
    "\n",
    "# Function to check if a URL is a direct PDF\n",
    "def is_pdf(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=10)\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "        if not content_type or \"application/pdf\" not in content_type:\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "        return \"application/pdf\" in content_type\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False \n",
    "\n",
    "# Function to extract PDF link from a webpage\n",
    "def extract_pdf_from_page(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  \n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Try extracting direct PDF links\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            text = link.text.strip().lower()\n",
    "            if \"view job order\" in text or link[\"href\"].endswith(\".pdf\"):\n",
    "                return urljoin(url, link[\"href\"])\n",
    "\n",
    "        # Look for PDFs in <iframe>\n",
    "        iframe = soup.find(\"iframe\", src=True)\n",
    "        if iframe and iframe[\"src\"].endswith(\".pdf\"):\n",
    "            return urljoin(url, iframe[\"src\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing webpage {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Function to download a PDF and track success\n",
    "def download_pdf(pdf_url, source_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True, timeout=10)\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "        if \"application/pdf\" not in content_type:\n",
    "            print(f\"⚠️ Not a PDF: {pdf_url} (Content-Type: {content_type})\")\n",
    "            failed_urls.append(source_url)\n",
    "            return\n",
    "\n",
    "        filename = os.path.basename(pdf_url)\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            filename += \".pdf\"  \n",
    "\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Write PDF file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            print(f\"✅ Downloaded: {file_path}\")\n",
    "            successful_downloads.append(file_path)\n",
    "        else:\n",
    "            print(f\"❌ Failed to save PDF properly: {file_path}\")\n",
    "            failed_urls.append(source_url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error downloading PDF from {source_url}: {e}\")\n",
    "        failed_urls.append(source_url)\n",
    "\n",
    "# Function to process each URL\n",
    "def process_url(url):\n",
    "    global driver\n",
    "    try:\n",
    "        if is_pdf(url):  \n",
    "            download_pdf(url, url)\n",
    "        else:\n",
    "            pdf_url = extract_pdf_from_page(url)\n",
    "            if pdf_url:\n",
    "                download_pdf(pdf_url, url)\n",
    "            else:\n",
    "                print(f\"⚠️ No 'View Job Order' PDF found on webpage: {url}\")\n",
    "                failed_urls.append(url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ WebDriver error, restarting: {e}\")\n",
    "        driver.quit()\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Use multi-threading for faster downloads\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    executor.map(process_url, urls)\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "\n",
    "# Print final download summary\n",
    "print(\"\\n📊 **Download Summary**\")\n",
    "print(f\"✅ Successfully downloaded PDFs: {len(successful_downloads)}\")\n",
    "print(f\"❌ Failed URLs: {len(failed_urls)}\\n\")\n",
    "\n",
    "if successful_downloads:\n",
    "    print(\"✅ **Successfully downloaded PDFs:**\")\n",
    "    for file in successful_downloads:\n",
    "        print(file)\n",
    "\n",
    "if failed_urls:\n",
    "    print(\"\\n⚠️ **Failed to download PDFs from these URLs:**\")\n",
    "    for url in failed_urls:\n",
    "        print(url)\n",
    "\n",
    "# Debugging output\n",
    "print(\"\\n🔍 Debugging successful_downloads list:\")\n",
    "print(successful_downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 **Download Comparison**\n",
      "Total URLs in hit.csv: 33\n",
      "Total PDFs downloaded: 140\n",
      "❌ Missing PDFs: 7\n",
      "\n",
      "⚠️ **Missing PDFs (Expected but Not Found):**\n",
      "H-300-19044-646927.pdf\n",
      "H-300-19114-230380.pdf\n",
      "H-300-19068-891355.pdf\n",
      "H-300-19106-808682.pdf\n",
      "H-300-19051-356435.pdf\n",
      "H-300-18254-022527.pdf\n",
      "H-300-23028-734237.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load CSV file\n",
    "file_path = \"hit.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract URLs from the \"Link to H-2A\" column\n",
    "column_name = \"Link to H-2A\"\n",
    "urls = df[column_name].dropna().unique()\n",
    "\n",
    "# Extract expected filenames from URLs\n",
    "expected_filenames = []\n",
    "for url in urls:\n",
    "    parsed_url = urlparse(url)\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        filename += \".pdf\"  # Ensure consistency with saved files\n",
    "    expected_filenames.append(filename)\n",
    "\n",
    "# Get actual downloaded files in H2A_PDFs/\n",
    "downloaded_files = set(os.listdir(\"H2A_PDFs\"))\n",
    "\n",
    "# Identify missing files\n",
    "missing_files = [filename for filename in expected_filenames if filename not in downloaded_files]\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 **Download Comparison**\")\n",
    "print(f\"Total URLs in hit.csv: {len(expected_filenames)}\")\n",
    "print(f\"Total PDFs downloaded: {len(downloaded_files)}\")\n",
    "print(f\"❌ Missing PDFs: {len(missing_files)}\\n\")\n",
    "\n",
    "if missing_files:\n",
    "    print(\"⚠️ **Missing PDFs (Expected but Not Found):**\")\n",
    "    for filename in missing_files:\n",
    "        print(filename)\n",
    "else:\n",
    "    print(\"✅ All expected PDFs were downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know 2 links are broken:\n",
    "  1. H-300-19106-808682\n",
    "  2. H-300-23028-734237\n",
    "\n",
    "\n",
    "That leaves 5 missing PDFs to account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will try and use beautiful soup to scrape information from the pdf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting BeautifulSoup\n",
      "  Using cached BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/6t/t91z119x51q_lpb_lspp0sjw0000gn/T/pip-build-env-6x_139ew/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/6t/t91z119x51q_lpb_lspp0sjw0000gn/T/pip-build-env-6x_139ew/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 304, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/6t/t91z119x51q_lpb_lspp0sjw0000gn/T/pip-build-env-6x_139ew/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 522, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/6t/t91z119x51q_lpb_lspp0sjw0000gn/T/pip-build-env-6x_139ew/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 3\n",
      "  \u001b[31m   \u001b[0m     \"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"<>\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n",
      "  \u001b[31m   \u001b[0m                                                                                                    ^^\n",
      "  \u001b[31m   \u001b[0m SyntaxError: invalid syntax\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 #- > Read and parse your content pdf\n",
    "%pip install requests #- > request for get the pdf\n",
    "%pip install BeautifulSoup # - > for parse the html and find all url hrf with \".pdf\" final\n",
    "from PyPDF2 import PdfFileReader\n",
    "import requests\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 117168_order.pdf: [Errno 22] Invalid argument\n",
      "Error processing 116897_order.pdf: [Errno 22] Invalid argument\n",
      "Error processing 110349_order.pdf: [Errno 22] Invalid argument\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(216, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(222, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(225, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(228, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(231, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(234, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(237, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(240, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(243, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(246, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(249, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(252, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(255, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(258, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(261, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(264, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(267, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(270, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(273, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(276, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(279, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(282, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(285, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(288, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(291, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(294, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(297, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(300, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(303, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(306, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(309, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(312, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(216, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(222, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(225, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(228, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(231, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(234, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(237, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(240, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(243, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(246, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(249, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(252, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(255, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(258, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(261, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(264, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(267, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(270, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(273, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(276, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(279, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(282, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(285, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(288, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(291, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(294, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(297, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(300, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(303, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(306, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(309, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(312, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(219, 0, 4647475600)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to extracted_text.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Define the folder path and output CSV file\n",
    "pdf_folder = \"H2A_PDFs\"  # Replace with the actual path if needed\n",
    "output_csv = \"extracted_text.csv\"\n",
    "\n",
    "# List all PDF files in the folder\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "# Store extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through and process each PDF\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            \n",
    "            # Extract text from each page\n",
    "            text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "            \n",
    "            # Append data to list\n",
    "            data.append({\"filename\": pdf_file, \"text\": text})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Extracted text saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data saved to extracted_html_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the folder path where HTML files are stored\n",
    "html_folder = \"/Users/shirleyaraizasantaella/Documents/UMich/Grad School Year 3/Semester 2/SI 699 - MSI Mastery/mlaw-699/H2A_PDFs\"  # Replace with the actual folder path\n",
    "\n",
    "# List all HTML files in the folder\n",
    "html_files = [f for f in os.listdir(html_folder) if f.endswith('.html')]\n",
    "\n",
    "# Loop through and process each HTML file\n",
    "for html_file in html_files:\n",
    "    html_path = os.path.join(html_folder, html_file)\n",
    "\n",
    "    # Open and read the HTML file\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    print(f\"\\nExtracting data from: {html_file}\\n\")\n",
    "   # Extract text content\n",
    "    text = \" \".join([p.text for p in soup.find_all('p')])\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "    data.append({\"filename\": html_file, \"text\": text, \"links\": \"; \".join(links)})\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"extracted_html_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Extracted data saved to extracted_html_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
